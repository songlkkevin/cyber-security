# Many-shot Jailbreaking

由于OpenAI， DeepMind等部署的模型提供了更场的上下文窗口。论文中提出了一个通过将few-shot Jailbreaking 扩展为Many-shot Jailbreaking的大模型攻击方法。通过提供大量的违反大模型规定的上下文如欺骗，提供可能对人类有伤害的信息诱导大模型对之后的问题给出开发者并不想让其给出的答案。从而使一个本来被设计为无害的模型变为有害。

对于目前部署的大模型，Many-shot Jialbreaking攻击的有效性遵循幂律规律。我们这种攻击对最先进的闭源模型以及各种任务的效果非常好。研究结果表明，更长的上下文为大模型攻击提供了更多的选择。

研究发现该攻击有很强的普适性。在不同模型不同任务:(1) Malicious use-cases (2)Malevolent personality evals，(3)Opportunities to insult。不同的对话风格:(1)变换user和assistant的标签（将二者身份倒置） (2)使用不同语言 (3)将user-assitant的标签换为"Question"与"Answer"等等。另外即使提供的上下文与最后想要问的问题为不同的领域同样有效。这让我想到了大模型中经典的prompt chain of thought，在最近的研究中prompt即使将原先的文字换位无意义的字符只要句子结构不改变同样能大幅提升模型的性能。

本篇文章还提出了将Many-shot Jailbreaking与(1)a blackbox, “competing objectives” attack (2) a white-box attack adversarial suffix attack结合提升Many-shot Jailbreaking的效果。结果表明与（1）结合在任何上下文长度都可以提升性能，而与（2）结合在则与shots的数量有关。

对于Many-shot Jailbreaking的效果大致遵循以下趋势：(1)随着shots的数量上升回答出攻击者期望答案的概率按指数上升 (2)更大的模型需要更少的shots即可攻击成功。另外作者提出了一个新的评估指标，即对$-log P(a^(*)|q_1,a_1,...q_n,a_n,q^(*))$也就是对于给定的上下文和问题，模型给出的答案的概率的负对数的期望。这个指标可以用来评估模型的安全性。截距说明了在无上下文中给出不期望答案的概率，斜率表明了给出不期望答案的概率随着上下文的增加而增加的速度。

论文的最后作者提出了一些分防御Many-shot Jailbreaking的方法。

第一种方法使通过SL或RL基于人类和AI的对话对大模型对齐，该方法通过让大模型通过在人和机器的对话中(无害的对话)进行监督学习或强化学习。该方法只能增大曲线的截距。这样使Jailbreaking成功需要更多的shots，该手段加上上下文长度限制可以一定程度防御Many-shot Jailbreaking。然而Many-shot Jailbreaking与其它方法结合如前文提到的black box等将会使这种防御失效。

第二种方法是对数据集进行监督训练微调，该方法通过让大模型在30个MSJ事例进行监督学习。这种方法既能增大曲线的截距也能减小曲线的斜率，但是并不能改变大模型给出不期望答案的趋势，该趋势仍是指数级别的。所以该防御方法仍然是不够有效的。

第三种方法是通过有目标的强化学习进行。与第一种方法类似，只不过在无害的shots中添加一部分有害的问题以诱导大模型说出不期望的答案。若说出不期望的答案则给予惩罚。这种方法可以有效的减小曲线的斜率，但是并不能改变大模型给出不期望答案的趋势。

第四种方法是基于prompt的。可以在输入prompt之前就对其进行修改，使其不会引导大模型给出不期望的答案。这个策略需要大量的测试，本论文仅对可行性进行了探索。ICD方法是在提示之前添加拒绝有害问题的演示，而CWD方法是在用户输入的文本之前和之后添加文本警告打不行不要Jailbreaking。ICD仅将攻击成功率从61\%降至54\%，而CWD降低了2\%的性能。所以在使用该方法之前需要对其的影响有更详细的研究。